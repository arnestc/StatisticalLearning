{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Lab 1: k-NN classifier\n",
    "\n",
    "## Exercise 2 – Phoneme recognition\n",
    "\n",
    "In this exercise, a real dataset will be employed. The dataset (file `speech_dataset.mat` in `/data/` folder) contains 5 features for each window of speech signal, with the aim to distinguish between nasal (class `1`) and oral sounds (class `2`). The five features are the normalized amplitudes of the five first harmonics of the speech signal. More information can be found here: https://www.openml.org/d/1489.\n",
    " \n",
    "The data set contains features for 5404 speech samples (the 6-th column is the class label). The dataset has to be divided into training and test set. The activity to be done is the same as for the previous exercise. Make sure you do not use too many values of K, as computations might take a lot of time.\n",
    "\n",
    "**Task**: your task is to implement a k-NN classifier in Matlab, which calculates the probability that a given test example belongs to each class, and outputs a class label as the class with the highest  probability. You will evaluate the classifier performance computing the average classification accuracy (i.e. the fraction of test examples that have been classified correctly).\n",
    " \n",
    "In particular, you should perform the following:\n",
    "* Split dataset into training and test set.\n",
    "* Train a k-NN classifier for different values of k.\n",
    "* Compare accuracy on the training set and the test set. Calculating accuracy of the training  set means that you will have to classify each sample in the training set as if it were a test sample; one expects that classification of training samples will perform well, and this may also be used to validate your implementation.\n",
    "    * Accuracy is defined as the ratio between the number of test samples that are correctly classified, and the total number of test samples.\n",
    "* Identifying overfitting and underfitting in the obtained results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "1. Holdout validation: it is a simple cross-validation approach to assess perfomance of the **kNN classification** model. It involves dividing the available dataset into two distinct subsets: a training set and a holdout set. The training set is used to train the machine learning model, while the holdout set is kept separate and only used after training to evaluate the model's performance on unseen data. Since the dataset is split into only two sets, the model is built just one time on the training set and executed faster.\n",
    "    * **Drawback**\n",
    "        * Single random split: different random splits may lead to different results, making it difficult to assess the true generalization ability of the model.\n",
    "        * Underestimation of the bias of the model: holdout validation may underestimate the tendency to consistently favor one class over another. This is because the holdout set is not used during model training, and the model's bias may be inadvertently reflected in the training data.\n",
    "        * Sensitivity to data distribution: this approach assumes that the data distribution is both constant and known. However, in real-world applications, data distribution may change over time or across different environments. Holdout validation may not be effective in these cases.\n",
    "        * Unbalanced dataset: in the case of unbalanced datasets, one class has significantly fewer examples than the other classes. Then, holdout validation can be particularly problematic because the model may be overfitted to the majority class, leading to poor performance on the minority class when evaluated on the holdout set.\n",
    "            * Dividing the dataset into a 70-30 training-testing split while preserving the original distribution of the two classes is a valid approach for holdout validation with imbalanced datasets. This ensures that the training set contains a representative sample of both classes, allowing the model to learn effectively from both majority and minority class data. The testing set also maintains the original class distribution, providing a realistic assessment of the model's generalization performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Plots\n",
    "sns.set_context(\n",
    "    'talk', rc = {\n",
    "        'font.size': 12.0,\n",
    "        'axes.labelsize': 10.0,\n",
    "        'axes.titlesize': 10.0,\n",
    "        'xtick.labelsize': 10.0,\n",
    "        'ytick.labelsize': 10.0,\n",
    "        'legend.fontsize': 10.0,\n",
    "        'legend.title_fontsize': 12.0,\n",
    "        'patch.linewidth': 2.0\n",
    "        }\n",
    "    ) # 'paper'\n",
    "\n",
    "data_sets = ['Train', 'Test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current folder\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/ernestocolacrai/Documents/GitHub/StatisticalLearning/data/\"\n",
    "\n",
    "try:\n",
    "    data = scipy.io.loadmat(data_path + f\"speech_dataset.mat\")\n",
    "    print(f\"Data ✓\")\n",
    "    print(f\"Data Keys: {data.keys()}\")\n",
    "except:\n",
    "    print(f\"Not found data! ({data_path})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = {\n",
    "    0: 'f1',\n",
    "    1: 'f2',\n",
    "    2: 'f3',\n",
    "    3: 'f4',\n",
    "    4: 'f5',\n",
    "    5: 'label'\n",
    "}\n",
    "\n",
    "data_df = pd.DataFrame({data_cols[c]: data['dataset'][:,c] for c in np.arange(0,6,1)})\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.label = data_df.label.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sqrt(np.sum((data_df.loc[0][:-1] - data_df.loc[1][:-1]) ** 2))\n",
    "len(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[data_df.duplicated(keep='first')]\n",
    "\n",
    "data_df = data_df.drop_duplicates()\n",
    "len(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Distribution for labels (0, 1):\\n\",\n",
    "    f\"\\tdataset: {(data_df.label.value_counts() / len(data_df)).values}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_holdout_split(df:pd.DataFrame, bin_feature:str, frac=0.67, s=1, verbose=False) -> tuple:\n",
    "\n",
    "    \"\"\"\n",
    "    This function splits a pandas DataFrame into a training and testing set while preserving the original distribution of a binary class represented by the `bin_feature` column.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): A pandas DataFrame containing the data.\n",
    "        bin_feature (str): The name of the column representing the binary class.\n",
    "        frac (float, optional): The fraction of rows to be sampled for the training set. The default value is 0.67, which means that 67% of the rows will be used for training and the remaining 33% for testing.\n",
    "        s (int, optional): The random seed for reproducibility. The default value is 1.\n",
    "        verbose (bool, optional): A boolean flag indicating whether to print additional information about the splitting process. The default value is False.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The training set as a pandas DataFrame.\n",
    "        pd.DataFrame: The testing set as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure that the 'bin_feature' column exists in the DataFrame.\n",
    "    assert bin_feature in df.columns, f\"Binary feature should be a data frame column.\"\n",
    "\n",
    "    # Check that 'frac' is less than 1 to prevent oversampling the data.\n",
    "    assert frac < 1, f\"Fraction of sampled rows should be smaller than 1 (100%).\"\n",
    "    \n",
    "    # Set the random seed for consistent splitting.\n",
    "    np.random.seed(s)\n",
    "\n",
    "    # Calculate the class distribution for the 'bin_feature' column.\n",
    "    classes = data_df[bin_feature].value_counts() / len(data_df)\n",
    "\n",
    "    train_set = []  # List to store the sampled data for the training set\n",
    "    test_set = []  # List to store the remaining data for the testing set\n",
    "\n",
    "    for class_label in tqdm(classes.index, colour='green'):\n",
    "        class_data = df[df[bin_feature] == class_label]\n",
    "\n",
    "        # Sample 67% of the data for the training set.\n",
    "        sampled_data = class_data.sample(frac=frac, random_state=s)\n",
    "        train_set.extend(sampled_data.values) # Add the sampled data to the training set.\n",
    "\n",
    "        # Remaining data goes to testing set.\n",
    "        remaining_data = class_data.drop(sampled_data.index)\n",
    "        test_set.extend(remaining_data.values) # Add the remaining data to the testing set.\n",
    "\n",
    "    # Rename data frames columns and set class type to int\n",
    "    data_cols = {}\n",
    "\n",
    "    for i in np.arange(0, len(data_df.columns), 1):\n",
    "        data_cols[i] = data_df.columns[i]\n",
    "\n",
    "    train_df = pd.DataFrame(train_set).rename(columns=data_cols)\n",
    "    train_df[train_df.columns[-1]] = train_df[train_df.columns[-1]].astype(int)\n",
    "\n",
    "    test_df = pd.DataFrame(test_set).rename(columns=data_cols)\n",
    "    test_df[test_df.columns[-1]] = test_df[test_df.columns[-1]].astype(int)\n",
    "\n",
    "    # Print information about the split data if 'verbose' is True.\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Datasets sizes:\\n\",\n",
    "            f\"\\tdataset: {len(df)}\\n\",\n",
    "            f\"\\ttrain set: {len(train_df)}\\n\",\n",
    "            f\"\\ttest set: {len(test_df)}\\n\\n\",\n",
    "            f\"\\t{'Correct sizes ✓' if len(train_df) + len(test_df) == len(df) else 'Wrong sizes ✕'}\\n\",\n",
    "            f\"Distribution for labels (0, 1):\\n\",\n",
    "            f\"\\tdataset: \\t{(df[df.columns[-1]].value_counts() / len(df)).values}\\n\",\n",
    "            f\"\\ttrain set: \\t{(train_df[train_df.columns[-1]].value_counts() / len(train_df)).values}\\n\",\n",
    "            f\"\\ttest set: \\t{(test_df[test_df.columns[-1]].value_counts() / len(test_df)).values}\"\n",
    "        )\n",
    "        \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = data_holdout_split(data_df, 'label', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(point1: np.ndarray, point2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the Euclidean distance between two points represented by NumPy arrays.\n",
    "\n",
    "    Parameters:\n",
    "        point1 (np.ndarray): The first point represented by a NumPy array.\n",
    "        point2 (np.ndarray): The second point represented by a NumPy array.\n",
    "\n",
    "    Returns:\n",
    "        float: The Euclidean distance between the two points.\n",
    "    \"\"\"\n",
    "\n",
    "    # Return the square root of the sum of squared distances.\n",
    "    return np.sqrt(np.sum((point1 - point2) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "\n",
    "M = len(test_df)\n",
    "N = len(train_df)\n",
    "\n",
    "pred = np.zeros(M, dtype=int)\n",
    "\n",
    "x_train = train_df.iloc[:,:-1].to_numpy()\n",
    "y_train = train_df.iloc[:,-1].to_numpy()\n",
    "\n",
    "x_test = test_df.iloc[:,:-2].to_numpy()\n",
    "\n",
    "D = np.zeros([M, N], dtype=float)  # Distance matrix\n",
    "E = np.zeros([M, k], dtype=int)  # Array of nearest neighbors\n",
    "\n",
    "for i in tqdm(np.arange(0, M, 1), colour='green'):  # For each test point\n",
    "    for j in np.arange(0, N, 1):  # For each training point\n",
    "        D[i][j] = np.sqrt(np.sum((x_test[i] - x_train[j]) ** 2)) # euclidean(x_test[i], x_train[j])\n",
    "    \n",
    "    E[i] = np.argsort(D[i])[:k]\n",
    "\n",
    "    l1 = np.sum(y_train[E[i]] == 1) >= (k + 1) / 2\n",
    "    l2 = np.sum(y_train[E[i]] == 2) >= (k + 1) / 2\n",
    "\n",
    "    if l1:\n",
    "        np.append(pred, 1)\n",
    "    elif l2:\n",
    "        np.append(pred, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_from_scratch(x_train: np.ndarray, y_train: np.ndarray, x_test: np.ndarray, k: int = 3, bar: bool = False) -> np.ndarray:\n",
    "\n",
    "    M = len(x_test)\n",
    "    N = len(x_train)\n",
    "\n",
    "    pred = np.zeros(M, dtype=int)\n",
    "\n",
    "    # x_train = train_df.iloc[:,:-1].to_numpy()\n",
    "    # y_train = train_df.iloc[:,-1].to_numpy()\n",
    "\n",
    "    # x_test = test_df.iloc[:,:-2].to_numpy()\n",
    "\n",
    "    D = np.zeros([M, N], dtype=float)  # Distance matrix\n",
    "    E = np.zeros([M, k], dtype=int)  # Array of nearest neighbors\n",
    "\n",
    "    for i in tqdm(np.arange(0, M, 1), colour='green', disable=bar):  # For each test point\n",
    "        for j in np.arange(0, N, 1):  # For each training point\n",
    "            D[i][j] = np.sqrt(np.sum((x_test[i] - x_train[j]) ** 2)) # euclidean(x_test[i], x_train[j])\n",
    "        \n",
    "        E[i] = np.argsort(D[i])[:k]\n",
    "\n",
    "        l1 = np.sum(y_train[E[i]] == 1) >= (k + 1) / 2\n",
    "        #l2 = np.sum(y_train[E[i]] == 2) >= (k + 1) / 2\n",
    "\n",
    "        prediction = 1 if (np.sum(y_train[E[i]] == 1) >= (k + 1) / 2) else 2\n",
    "        pred[i] = prediction  # Update prediction array with the calculated value\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_accuracy(test_df: pd.DataFrame) -> float:\n",
    "    \n",
    "    return 1 - len(test_set[test_set.label != test_set.prediction]) / len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = len(x_test)\n",
    "N = len(x_train)\n",
    "\n",
    "pred = np.zeros(M, dtype=int)\n",
    "\n",
    "x_train = train_df.iloc[:,:-1].to_numpy()\n",
    "y_train = train_df.iloc[:,-1].to_numpy()\n",
    "\n",
    "x_test = test_df.iloc[:,:-2].to_numpy()\n",
    "\n",
    "D = np.zeros([M, N], dtype=float)  # Distance matrix\n",
    "E = np.zeros([M, k], dtype=int)  # Array of nearest neighbors\n",
    "\n",
    "for i in tqdm(np.arange(0, M, 1), colour='green'):  # For each test point\n",
    "    for j in np.arange(0, N, 1):  # For each training point\n",
    "        D[i][j] = np.sqrt(np.sum((x_test[i] - x_train[j]) ** 2)) # euclidean(x_test[i], x_train[j])\n",
    "        \n",
    "    E[i] = np.argsort(D[i])[:k]\n",
    "\n",
    "    l1 = np.sum(y_train[E[i]] == 1) >= (k + 1) / 2\n",
    "    l2 = np.sum(y_train[E[i]] == 2) >= (k + 1) / 2\n",
    "\n",
    "    if l1:\n",
    "        pred[i] = 1\n",
    "    elif l2:\n",
    "        pred[i] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['prediction'] = knn_from_scratch(\n",
    "    x_train=train_df.iloc[:,:-1].to_numpy(),\n",
    "    y_train=train_df.iloc[:,-1].to_numpy(),\n",
    "    x_test=test_df.iloc[:,:-2].to_numpy(),\n",
    "    k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_vals = np.arange(1, 50, 2)\n",
    "misclass_errs = {}\n",
    "for d_type in data_sets:\n",
    "    misclass_errs[d_type] = []\n",
    "    print(f\"{d_type} set misclassification error:\")\n",
    "    for k in tqdm(k_vals, colour=\"green\"):\n",
    "    #for k in k_vals:\n",
    "        if d_type == 'Train':\n",
    "            pred = knn_from_scratch(\n",
    "                x_train=train_df.iloc[:,:-1].to_numpy(),\n",
    "                y_train=train_df.iloc[:,-1].to_numpy(),\n",
    "                x_test=train_df.iloc[:,:-1].to_numpy(),\n",
    "                k=k)\n",
    "            misclass_errs[d_type].append(np.sum(train_df.label.values != pred) / len(train_df))\n",
    "        else:\n",
    "            pred = knn_from_scratch(\n",
    "                x_train=train_df.iloc[:,:-1].to_numpy(),\n",
    "                y_train=train_df.iloc[:,-1].to_numpy(),\n",
    "                x_test=test_df.iloc[:,:-2].to_numpy(),\n",
    "                k=k)\n",
    "            misclass_errs[d_type].append(np.sum(train_df.label.values != pred) / len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for d_type in data_sets:\n",
    "    sns.lineplot(\n",
    "        x=k_vals, y=misclass_errs[d_type],\n",
    "        ax=ax,\n",
    "        label=f\"{d_type} set\"\n",
    "    )\n",
    "    \n",
    "ax.legend(framealpha=0)\n",
    "ax.set_ylabel(\"misclassification rate\")\n",
    "ax.set_xlabel(\"k parameter\")\n",
    "ax.set_title(\"with methods from scratch (not using sklearn)\")\n",
    "\n",
    "sns.despine(right=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(train_df: pd.DataFrame, test_df: pd.DataFrame, k: int = 3):\n",
    "    # Initialize data structures\n",
    "    M = len(test_set)  # Number of data points in the training set\n",
    "    N = len(train_set)  # Number of data points in the test set\n",
    "\n",
    "    # Validate k parameter\n",
    "    assert (type(k) != float) and (k % 2 == 1), \"k parameter should be an odd integer number.\"\n",
    "    assert k < N, \"k parameter should be smaller than the train set size.\"\n",
    "\n",
    "    D = np.zeros([M, N], dtype=float)  # Distance matrix\n",
    "    E = np.zeros([M, k], dtype=int)  # Array of nearest neighbors\n",
    "\n",
    "    # Calculate distances between test set points and training set points\n",
    "    for i in tqdm(np.arange(0, M, 1), colour='green'):  # For each test point\n",
    "        for j in np.arange(0, N, 1):  # For each training point\n",
    "            D[i][j] = euclidean(  # Calculate distance between the points\n",
    "                test_set.loc[i][:-2].values,  # Current test point coordinates\n",
    "                train_set.loc[j][:-1].values  # Corresponding training point coordinates\n",
    "            )\n",
    "\n",
    "        # Find the k nearest neighbors for each test point\n",
    "        E[i] = np.argsort(D[i])[:k]  # Obtain the indices of the k nearest neighbors\n",
    "    \n",
    "        # Assign predictions based on majority of labels among the k nearest neighbors\n",
    "        l1 = np.sum([int(train_set.loc[E[i][idx]][-1] == 1) for idx in np.arange(0, k)]) >= (k + 1) / 2\n",
    "        l2 = np.sum([int(train_set.loc[E[i][idx]][-1] == 2) for idx in np.arange(0, k)]) >= (k + 1) / 2\n",
    "\n",
    "        if l1:\n",
    "            test_set.loc[i, 'prediction'] = 1\n",
    "        elif l2:\n",
    "            test_set.loc[i, 'prediction'] = 2\n",
    "\n",
    "    return test_set  # Return the modified test set with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_accuracy(test_set: pd.DataFrame) -> float:\n",
    "    return 1 - len(test_set[test_set.label != test_set.prediction]) / len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation by using sklearn\n",
    "k_vals = np.arange(1, 1000, 2)\n",
    "x = k_vals\n",
    "y = {}\n",
    "\n",
    "for d_type in data_sets:\n",
    "    print(f\"{d_type} set:\")\n",
    "    \n",
    "    y[d_type] = []\n",
    "\n",
    "    for k in tqdm(k_vals, colour='green'):\n",
    "        # Instantiate learning model\n",
    "        neig = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "        # Fitting the model\n",
    "        neig.fit(train_df[['f1', 'f2', 'f3', 'f4', 'f5']].values, train_df['label'].values)\n",
    "        \n",
    "        # Predict the response\n",
    "        if d_type == 'Train':\n",
    "            pred = neig.predict(train_df[['f1', 'f2', 'f3', 'f4', 'f5']].values)\n",
    "\n",
    "            # Evaluate misclassification error\n",
    "            y[d_type].append(1 - accuracy_score(train_df.label.values, pred))\n",
    "        else:\n",
    "            pred = neig.predict(test_df[['f1', 'f2', 'f3', 'f4', 'f5']].values)\n",
    "\n",
    "            # Evaluate misclassification error\n",
    "            y[d_type].append(1 - accuracy_score(test_df.label.values, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, ax2 = plt.subplots()\n",
    "\n",
    "for d_type in data_sets:\n",
    "    sns.lineplot(\n",
    "        x=k_vals, y=y[d_type],\n",
    "        ax=ax2,\n",
    "        label=f\"{d_type} set\"\n",
    "    )\n",
    "    \n",
    "ax2.legend(framealpha=0)\n",
    "ax2.set_ylabel(\"misclassification rate\")\n",
    "ax2.set_xlabel(\"k parameter\")\n",
    "ax2.set_title(\"with sklearn methods\")\n",
    "\n",
    "sns.despine(right=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

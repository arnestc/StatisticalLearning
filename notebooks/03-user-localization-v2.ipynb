{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Lab 1: k-NN classifier\n",
    "\n",
    "## Exercise 3 – User localization from RSSI\n",
    "\n",
    "Consider the following scenario, in which we wish to localize a user employing a non-GPS system  (e.g., in indoor localization). The user holds a transmission device (e.g., a smartphone or other sensor with transmission capabilities). Localization is based on measurements of the Received Signal Strength Indicator (RSSI) from D sensors (base stations) placed in the area in which the localization service is provided. The area is divided into $N_C$ square cells, and localization amounts to identifying the cell in which the user is located.\n",
    "\n",
    "In a **training stage**, the transmission device is placed in the center of each cell and broadcasts a data packet, and RSSI is measured by each sensor. This yields one measurement, corresponding to a vector of length $D$. The process is repeated $M$ times for each cell, and for all $N_C$ cells. The training stage provides a 3-dimensional array of size $N_C \\times D \\times M$.\n",
    "\n",
    "In a **test stage**, the user is located in an unknown cell. The transmission device broadcasts a data packet, and each sensor measures the RSSI and communicates it to a fusion center. The fusion center treats the received RSSI values as a test vector of length $D$. It applies a k-NN classifier, comparing the test vector with all $M \\times N_C$ training vectors available in the training set. For each test vector, the k-NN classifier outputs the probability that each cell contains the user.\n",
    "\n",
    "**Available data**: you are provided with a file (`localization.mat` in `/data/` folder) containing two variables, called traindata and testdata. These variables have the same size, and are 3-dimensional arrays of size $D=7$, $M=5$, and $N_C = 24$.\n",
    "\n",
    "The training data can be seen as labelled data where each cell is a class, and you are given M data vectors for each cell. Regarding the test data, a test vector consists of a single measurement; so each measurement has to be used individually and you can perform up to M tests for each cell.\n",
    "The data correspond to real acquisition experiments performed outdoors nearby Politecnico di Torino, using an STM32L microcontroller with 915 MHz 802.15.4 transceiver.\n",
    "\n",
    "**Task**: your task is to implement a k-NN classifier in Matlab for the classification task described above, and evaluate its performance.\n",
    "\n",
    "**Performance evaluation**: The performance is defined in terms of accuracy in the localization task, and it has to be averaged over all cells. Average accuracy is defined as the posterior probability associated to the cell that the user is actually located in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import scipy.io\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Plot Seaborn settings\n",
    "sns.set_context(\n",
    "    'talk',\n",
    "    rc = {\n",
    "        'font.size': 12.0,\n",
    "        'axes.labelsize': 10.0,\n",
    "        'axes.titlesize': 10.0,\n",
    "        'xtick.labelsize': 10.0,\n",
    "        'ytick.labelsize': 10.0,\n",
    "        'legend.fontsize': 10.0,\n",
    "        'legend.title_fontsize': 12.0,\n",
    "        'patch.linewidth': 2.0\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check current folder\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ✓\n",
      "Data Keys: dict_keys(['__header__', '__version__', '__globals__', 'cell_coordinates', 'testdata', 'traindata'])\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/ernestocolacrai/Documents/GitHub/StatisticalLearning/data/\"\n",
    "\n",
    "try:\n",
    "    # Attempt to load the MATLAB data file .mat\n",
    "    data = scipy.io.loadmat(path + \"localization.mat\")\n",
    "\n",
    "    print(\n",
    "        f\"Data ✓\\n\"\n",
    "        f\"Data Keys: {data.keys()}\"\n",
    "    )\n",
    "except:\n",
    "    print(f\"Not found data! ({path})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: \t(7, 5, 24), type: <class 'numpy.ndarray'>\n",
      "Test dataset shape: \t(7, 5, 24), type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Check train and test datasets shapes and types\n",
    "\n",
    "print(\n",
    "    f\"Train dataset shape: \\t{data['traindata'].shape}, type: {type(data['traindata'])}\\n\"\n",
    "    f\"Test dataset shape: \\t{data['testdata'].shape}, type: {type(data['testdata'])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 7 # Features number (ROWS)\n",
    "M = 5 # Measures number for each cell (class) (COLUMNS)\n",
    "Nc = 24 # Classes number (cells number) (DEPTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Rearrange data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange(dataset:np.ndarray, rows:int, columns:int, depth:int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reshapes a 3D NumPy array into a flattened 2D array.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (numpy.ndarray): A 3D NumPy array representing the input dataset.\n",
    "        rows (int): The number of rows in the input dataset.\n",
    "        columns (int): The number of columns in the input dataset.\n",
    "        depth (int): The number of depth dimensions in the input dataset.\n",
    "\n",
    "    Returns:\n",
    "        (numpy.ndarray): The rearranged dataset, represented as a 2-dimensional NumPy array with dimensions of `(columns * depth, rows + 1)`.\n",
    "    \"\"\"\n",
    "    arranged = np.zeros([columns * depth, rows + 1]) # Initialize an empty 2D NumPy array to store the rearranged data\n",
    "    label = 0 # Multi-class label (1,2,...,24 classes)\n",
    "    for j in range(depth): # Iterate through the depth\n",
    "        for i in range(columns): # Iterate through each column\n",
    "            # Rearrange the data from the input dataset along the depth dimension into a temporary 1D array\n",
    "            rearranged_data = dataset[:, i, j].T\n",
    "            arranged[i + label, :-1] = rearranged_data # Append the rearranged data to the `arranged` array\n",
    "            arranged[i + label, -1] = j + 1 # +1 since it starts from 0\n",
    "        \n",
    "        label += columns # Update the label within the `arranged` array\n",
    "    \n",
    "    # Return the final rearranged array\n",
    "    return arranged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the (rearranged) train and test datasets\n",
    "train_data = rearrange(data['traindata'], D, M, Nc).astype(int)\n",
    "test_data = rearrange(data['testdata'], D, M, Nc).astype(int)\n",
    "\n",
    "# Random permutation\n",
    "train_data = np.random.permutation(train_data)\n",
    "test_data = np.random.permutation(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: \t(120, 8), type: <class 'numpy.ndarray'>\n",
      "Test dataset shape: \t(120, 8), type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Check the (rearranged) train and test datasets shapes and types\n",
    "\n",
    "print(\n",
    "    f\"Train dataset shape: \\t{train_data.shape}, type: {type(train_data)}\\n\"\n",
    "    f\"Test dataset shape: \\t{test_data.shape}, type: {type(test_data)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. kNN classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 11 # Number of nearest-neighboors\n",
    "bar = True # Show tqdm progress bar\n",
    "\n",
    "M = len(test_data)\n",
    "N = len(train_data)\n",
    "\n",
    "D = np.zeros([M, N], dtype=float)  # Distance matrix\n",
    "E = np.zeros([M, k], dtype=int)  # Array of nearest neighbors\n",
    "\n",
    "infer_labels = np.zeros(M, dtype=int) # Inferred labels\n",
    "\n",
    "for i in tqdm(range(M), colour='green', disable=bar): # For each test point\n",
    "    for j in range(N): # For each training point\n",
    "        D[i][j] = np.sqrt(np.sum((test_data[i] - train_data[j]) ** 2)) # Calculate euclidean distance between the points\n",
    "    # Find indices of k nearest neighbors\n",
    "    E[i] = np.argsort(D[i])[:k]\n",
    "\n",
    "    infer_labels[i] = np.argmax(np.bincount(train_data[E[i]][:,-1].astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 9,  2,  5,  2, 13,  6, 21,  3, 14,  5, 13, 11,  9,  1, 17,  9,  1,\n",
       "        14,  2,  2, 14, 14, 21, 11, 17,  6, 22, 14, 20,  6,  2,  2,  2, 18,\n",
       "        18,  3,  6,  6,  6, 17, 11, 17, 20,  5, 16,  2,  6,  3, 18,  9, 18,\n",
       "         2,  6, 17, 13,  1, 20, 20,  6,  6, 21,  6, 22, 23,  6, 21,  6,  7,\n",
       "        22, 13,  5, 18, 18, 14, 23, 18,  7,  6, 11,  6,  9,  2, 18, 10, 20,\n",
       "        17,  9, 14,  6, 21, 10, 17,  9, 21,  2, 21, 13,  7, 22,  1, 14, 13,\n",
       "        23, 21, 22,  2, 14,  7, 21, 18, 18, 17, 23,  9, 21, 14,  6,  3,  9,\n",
       "        23]),\n",
       " array([[ 43, 108,  81, ...,  90,  30,  91],\n",
       "        [ 97,  95,   6, ...,  96,  32, 114],\n",
       "        [ 38,  58,  86, ..., 114,   8,  26],\n",
       "        ...,\n",
       "        [ 35, 113, 111, ...,   6,  62,  61],\n",
       "        [  7,  90,  45, ...,  71,  49,  43],\n",
       "        [112,  41,  18, ..., 105, 104,  65]]))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_labels, E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values: \t[5 6 7 8]\n",
      "Frequencies: \t[1 4 5 1]\n",
      "Probabilities: \t[0.0909 0.3636 0.4545 0.0909]\n",
      "\n",
      "Inferret label (with probability): (7, 0.4545)\n"
     ]
    }
   ],
   "source": [
    "# Test to return inferred label and non-zero probability for each class\n",
    "# random.seed(1)\n",
    "idx = random.randint(0,119) # Select a random istance of dataset\n",
    "\n",
    "values, frequencies = np.unique(train_data[E[idx]][:,-1], return_counts=True)\n",
    "total_labels_number = len(train_data[E[idx]][:,-1])\n",
    "probabilities = np.round(frequencies / total_labels_number, 4)\n",
    "\n",
    "print(\n",
    "    f\"Values: \\t{values}\\n\"\n",
    "    f\"Frequencies: \\t{frequencies}\\n\"\n",
    "    f\"Probabilities: \\t{probabilities}\\n\\n\"\n",
    "    f\"Inferret label (with probability): {values[np.argmax(frequencies)], probabilities[np.argmax(frequencies)]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-45, -35, -37, -49, -55, -67, -47,   7],\n",
       "       [-44, -34, -36, -50, -55, -68, -49,   7],\n",
       "       [-44, -35, -36, -50, -55, -68, -49,   7],\n",
       "       [-44, -34, -37, -50, -55, -72, -49,   7],\n",
       "       [-44, -34, -39, -53, -55, -68, -53,   5],\n",
       "       [-44, -36, -32, -48, -62, -58, -33,   8],\n",
       "       [-48, -34, -36, -52, -62, -56, -42,   6],\n",
       "       [-49, -33, -35, -53, -62, -57, -42,   6],\n",
       "       [-45, -35, -36, -49, -55, -80, -49,   7],\n",
       "       [-49, -33, -35, -54, -62, -57, -42,   6],\n",
       "       [-49, -34, -36, -53, -63, -57, -42,   6]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[E[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
